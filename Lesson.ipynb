{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6ChuqWNK-WX"
      },
      "outputs": [],
      "source": [
        "#si-exercise\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "class RegressionModel:\n",
        "    def __init__(self, x: pd.DataFrame, y: pd.DataFrame, create_intercept: bool, regression_type=\"ols\"):\n",
        "        self.x = x.copy()  # Store the exogenous variables\n",
        "        self.y = y.copy()  # Store the endogenous variable\n",
        "        self.create_intercept = create_intercept\n",
        "        self.regression_type = regression_type\n",
        "        self.results = {}\n",
        "\n",
        "        if self.create_intercept:\n",
        "            self.add_intercept()\n",
        "\n",
        "    def add_intercept(self):\n",
        "        # Adds a column of ones to x as the intercept\n",
        "        self.x['intercept'] = 1\n",
        "\n",
        "    def ols_regression(self):\n",
        "        # Convert the x DataFrame to a NumPy matrix\n",
        "        X = self.x.values\n",
        "        X = np.array(X, dtype=float)\n",
        "\n",
        "        # Ensure y is a 2D column vector\n",
        "        y = np.array(self.y).reshape(-1, 1)\n",
        "\n",
        "        # Performs ordinary least squares regression\n",
        "        XtX = np.dot(X.T, X)  # X'X\n",
        "        XtY = np.dot(X.T, self.y)  # X'y\n",
        "        beta_hat = sp.linalg.solve(XtX, XtY, assume_a='pos')\n",
        "\n",
        "        # Predicted values and residuals\n",
        "        y_hat = np.dot(X, beta_hat)\n",
        "        residuals = self.y - y_hat\n",
        "\n",
        "        # Calculate degrees of freedom and residual variance\n",
        "        n = len(self.y)  # number of rows\n",
        "        k = X.shape[1]  # number of columns\n",
        "        df = n - k\n",
        "        residual_variance = np.sum(residuals**2) / df\n",
        "\n",
        "        # Standard errors\n",
        "        var_beta_hat = residual_variance * sp.linalg.inv(XtX)  # Variance-covariance matrix of beta_hat\n",
        "        standard_errors = np.sqrt(np.diag(var_beta_hat))  # Standard errors\n",
        "\n",
        "        t_stats = beta_hat / standard_errors\n",
        "        p_values = sp.stats.t.sf(np.abs(t_stats), df)\n",
        "\n",
        "        # Store the regression results in a dictionary\n",
        "        variable_names = self.x.columns\n",
        "        for i, var in enumerate(variable_names):\n",
        "            self.results[var] = {\n",
        "                \"coefficient\": beta_hat[i],\n",
        "                \"standard_error\": standard_errors[i],\n",
        "                \"t_stat\": t_stats[i],\n",
        "                \"p_value\": p_values[i]\n",
        "            }\n",
        "\n",
        "    def logistic_regression(self, learning_rate=0.001, max_iter=10000, tol=1e-5, regularization=1e-6):\n",
        "        # Performs logistic regression using gradient descent\n",
        "        X = np.array(self.x, dtype=float)\n",
        "        y = np.array(self.y).reshape(-1, 1)\n",
        "        n, k = X.shape\n",
        "\n",
        "        # Initialize beta coefficients\n",
        "        beta = np.zeros((k, 1))\n",
        "\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def log_likelihood(beta):\n",
        "            z = np.dot(X, beta)\n",
        "            return np.sum(y * z - np.log(1 + np.exp(z)))\n",
        "\n",
        "        previous_log_likelihood = -np.inf\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            z = np.dot(X, beta)\n",
        "            predictions = sigmoid(z)\n",
        "            gradient = np.dot(X.T, (y - predictions))\n",
        "            beta_new = beta + learning_rate * gradient\n",
        "\n",
        "            current_log_likelihood = log_likelihood(beta_new)\n",
        "            if np.abs(current_log_likelihood - previous_log_likelihood) < tol:\n",
        "                break\n",
        "            beta = beta_new\n",
        "            previous_log_likelihood = current_log_likelihood\n",
        "\n",
        "        # Store coefficients as log odds-ratios (renamed to \"coefficient\" for compatibility)\n",
        "        beta_hat = beta.flatten()\n",
        "\n",
        "\n",
        "        # Regularized Hessian to ensure invertibility\n",
        "        predictions_flat = predictions.flatten()\n",
        "        hessian = -np.dot(X.T, X * (predictions_flat * (1 - predictions_flat)).reshape(-1, 1))\n",
        "        hessian += np.eye(k) * regularization  # Regularization to stabilize inversion\n",
        "\n",
        "        try:\n",
        "          var_beta_hat = np.linalg.inv(hessian)\n",
        "        except np.linalg.LinAlgError:\n",
        "          raise ValueError(\"Hessian matrix is singular and cannot be inverted. Try adjusting regularization.\")\n",
        "\n",
        "        standard_errors = np.sqrt(np.diag(var_beta_hat))\n",
        "        z_stats = beta_hat / standard_errors\n",
        "        p_values = sp.stats.norm.sf(np.abs(z_stats))\n",
        "\n",
        "        variable_names = self.x.columns\n",
        "        for i, var in enumerate(variable_names):\n",
        "            self.results[var] = {\n",
        "                \"coefficient\": beta_hat[i],\n",
        "                \"standard_error\": standard_errors[i],\n",
        "                \"z_stat\": z_stats[i],\n",
        "                \"p_value\": p_values[i]\n",
        "            }\n",
        "\n",
        "    def fit_model(self):\n",
        "        # Determines the type of regression to run based on regression_type attribute\n",
        "        if self.regression_type == \"ols\":\n",
        "            self.ols_regression()\n",
        "        elif self.regression_type == \"logit\":\n",
        "            self.logistic_regression()\n",
        "        else:\n",
        "            raise ValueError(\"Invalid regression type. Choose 'ols' or 'logit'.\")\n",
        "\n",
        "    def summary(self):\n",
        "        # Prints a summary table of regression results\n",
        "        if self.regression_type == \"ols\":\n",
        "            print(f\"{'Variable name':<15}{'Coefficient':<15}{'Standard Error':<15}{'t-statistic':<15}{'p-value':<15}\")\n",
        "            print(\"=\" * 75)\n",
        "            for var, metrics in self.results.items():\n",
        "                print(f\"{var:<15}{metrics['coefficient']:<15.4f}{metrics['standard_error']:<15.4f}\"\n",
        "                      f\"{metrics['t_stat']:<15.4f}{metrics['p_value']:<15.4f}\")\n",
        "        elif self.regression_type == \"logit\":\n",
        "            print(f\"{'Variable name':<15}{'Coefficient':<15}{'Standard Error':<15}{'z-statistic':<15}{'p-value':<15}\")\n",
        "            print(\"=\" * 75)\n",
        "            for var, metrics in self.results.items():\n",
        "                print(f\"{var:<15}{metrics['coefficient']:<15.4f}{metrics['standard_error']:<15.4f}\"\n",
        "                      f\"{metrics['z_stat']:<15.4f}{metrics['p_value']:<15.4f}\")"
      ]
    }
  ]
}